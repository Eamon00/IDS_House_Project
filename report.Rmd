---
title: "Predicting House Prices"
author: "by JIFEM: Joe, Eamon, Innes, Freddie & Mat"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load-lib, include = FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(corrplot)
library(dplyr)
library(purrr)

```


```{r load-data, include=FALSE}

house_prices <- read_csv("data/train.csv")

house_prices_filtered <- house_prices %>% 
  mutate(TotalBath = BsmtFullBath + 
           BsmtHalfBath + 
           HalfBath + 
           FullBath)


qual_map <- c("Ex" = 5,
              "Gd" = 4,
              "TA" = 3,
              "Fa" = 2,
              "Po" = 1)

pave_map <- c("Y" = 2,
              "N"= 0,
              "P"= 1)

electric_map <- c("SBrkr" = 5,
                  "FuseA" = 4,
                  "FuseF" = 3,
                  "FuseP" = 2,
                  "Mix" = 1)

building_map <- c("1Fam"   = 5,
                  "TwnhsE" = 4,
                  "Twnhs"  = 3,
                  "Duplex" = 2,
                  "2fmCon" = 1,
                  "Other"  = 0)


# Apply mapping
house_prices_filtered$KitchenQual <- qual_map[house_prices_filtered$KitchenQual]
house_prices_filtered$ExterQual <- qual_map[house_prices_filtered$ExterQual]
house_prices_filtered$HeatingQC <- qual_map[house_prices_filtered$HeatingQC]
house_prices_filtered$HeatingQC <- qual_map[house_prices_filtered$HeatingQC]
house_prices_filtered$PavedDrive <- pave_map[house_prices_filtered$PavedDrive]
house_prices_filtered$Electrical <- electric_map[house_prices_filtered$Electrical]
house_prices_filtered$BldgType <- building_map[house_prices_filtered$BldgType]

numeric_vars <- house_prices_filtered[, sapply(house_prices_filtered, is.numeric)]

price_corr <- cor(numeric_vars, use = "complete.obs")[, "SalePrice"]

price_corr_df <- data.frame(
  variable = names(price_corr),
  correlation = as.numeric(price_corr)
) %>%
  arrange(desc(correlation)) 

Price_corr_filtered <- price_corr_df %>%
  filter(correlation > 0.15) 


theme_house <- theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 15),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "white", colour = NA)
  )

main_colour <- "#3B82F6"   
accent_colour <- "#1E40AF" 
```


# Introduction

Our objective is to accurately predict house sale prices using a comprehensive set of explanatory variables from the Ames Housing dataset. Compiled by Dean De Cock, this dataset provides detailed information on residential properties sold in Ames, Iowa, between 2006 and 2010. The dataset is divided into training and testing sets, each containing the sale price for each property along with numerous features such as neighborhood, number of bathrooms and overall quality which can all be used to predict the properties sale price.

The goal of our investigation is to produce a model which can successfully predict the sale price of properties in the testing data set. We also aim to identify the variables which have the greatest predictive power for sale price.

Kaggle.com. (2016). House Prices: Advanced Regression Techniques | Kaggle. [online] Retrieved November 1, 2025, from https://www.kaggle.com/c/house-prices-advanced-regression-techniques.


# Methods

## Data Cleaning and Preprocessing

Before building our predictive model, we first performed data cleaning and preprocessing to ensure the dataset was suitable for analysis.

Our first step was to create a more useful representation of bathroom-related variables. Rather than treating — BsmtHalfBath, BsmtFullBath, HalfBath, and FullBath — as seperate variables; we combined them into a single variable, TotalBath, representing the total number of bathrooms in each house. This made comparisons across properties simpler as the dataset was more intuitive and simpler to model.

We also transformed the KitchenQual variable from categorical descriptions of quality into a numerical rating scale from 1 to 5, where 5 represents the highest quality and 1 the lowest. This conversion made the variable easier to interpret and more convenient to use in modelling functions. We applied the same approach to ExterQual, using the same qual_map transformation because both variables used identical categorical descriptors.

In addition, all NA values in the PoolArea variable were converted to 0. Rather than implying missing data, NA in this context simply indicates that the house does not have a pool. Treating these as zeros prevented the presence of missing values from complicating numeric functions or producing unexpected results during analysis.

We did not remove any outliers in terms of house price, under the assumption that the selected explanatory variables would be able to account for the full range of property values.

## Data Analysis

To familiarize ourselves with the dataset, we began by producing several exploratory graphs. For example, we plotted the distribution of sale prices to ensure there was sufficient variation to support meaningful prediction. 

We observed a roughly bell shaped distribution centred around $180,000. Although there was sufficient variation in house prices to allow for meaningful prediction, the high value of right skew could cause issues when modeling higher house prices due to the limited number of data points.

## Figure 1.1 - Distribution of house prices
```{r, echo=FALSE}
ggplot(house_prices_filtered, aes(x = SalePrice/1000)) +
  geom_histogram(
    bins = 30,
    fill = main_colour,
    color = accent_colour
  ) +
  labs(title = NULL,
       x = "Sale Price (Thousands of $)",
       y = "Count") +
  theme_house
```

The original dataset contained over 80 variables which introduced unnecessary noise and risked over fitting. Therefore, we chose to reduce the number of variables down to 15. Initially, we selected variables based on intuition and visual inspection—choosing features we believed would be strongly correlated with sale price. However, we quickly realised that a more systematic approach would be more efficient. We generated a correlation heatmap for all numerical variables and identified the 15 features with the highest correlations to the target variable. These variables formed the basis for our subsequent modelling work.

## Figure 1.2 - Variables correlation with Sale Price

```{r, echo=FALSE}
ggplot(Price_corr_filtered,
       aes(x = reorder(variable, correlation), y = correlation)) +
  geom_col(fill = main_colour, color = accent_colour) +
  coord_flip() +
  labs(
    title = NULL,
    x = "Variable",
    y = "Correlation"
  ) + theme_house +
  theme(
  axis.text.y = element_text(size = 6),
  plot.title = element_blank()
)
```


Using this heat map, we condensed our number of variables down to 15, by selecting a variety of unique variables which showed strong correlation with sale price.

We initally attempted to condense our variables down through intuition, by selecting variables we assumed would have high correlation with sale price. This did work okay, however we did some analysis on our variables, and found some surprising results. We were surprised to find that although Overall Quality had a positive correlation with Sale price (seen in figure 1.3), Overall Condition had a negative correlation with Sale price (seen in figure 1.4).



## Figure 1.3 and 1.4 - Variables correlation with Sale Price


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(readr)

# Load the data
df <- read_csv("data/train.csv", show_col_types = FALSE)

ggplot(df, aes(x = OverallQual, y = SalePrice / 1000)) +
  geom_jitter(alpha = 0.5, width = 0.2) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") + 
  labs(
    title = "Sale Price vs Overall Quality",
    x = "Overall Quality (1 = Poor, 10 = Excellent)",
    y = "Sale Price (Thousands of $)"
  ) +
  theme_house

ggplot(df, aes(x = OverallCond, y = SalePrice / 1000)) +
  geom_jitter(alpha = 0.5, width = 0.2) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") + 
  labs(
    title = "Sale Price vs Overall Condition",
    x = "Overall Condition (1 = Poor, 10 = Excellent)",
    y = "Sale Price (Thousands of $)"
  ) +
  theme_house

```

# MODEL + MODELLING METHODS

With 15 potentially useful variables identified, we needed to determine the optimal subset for prediction. Including all of the 15 variables risked overfitting, while using too few may miss important predictive variables. We chose to use linear regression as our modelling method for a multitude of reasons. Linear regression gives us easily interpretable coefficients, meaning we could not only predict sale price, but also see which variables were most important in the predictions. We also though the assumptions of linear regression, such as linearity and independance, seemed reasonable for our dataset based on our explanatory plots. We did consider using a random forest model, however we valued the interpretability of the linear regression model higher.

To systematically reduce our variable set, we decided to use a brute force, exhaustive method, of trying every single combination of 12 variables on our model, and recording the R^2 value of it. R^2 was chosen as it rewards models that explain variance efficiently.

We tested 455 models (15 choose 12,) to come up with the 12 best variables we could find. They were: GarageCars, BedroomAbvGr, TotalBath, BldgType, LotArea, Neighborhood, KitchenQual, TotalBsmtSF, OverallQual, ExterQual, YearBuilt, GrLivArea. Comparing this to the correlation heatmap, we can see these are not just the 12 variables with the highest correlation, meaning that they would have produced worse results. When running this combination of variables with an 80:20 train:test split, we produced a model with an R^2 of 0.8513 and an MAE of $22073.69. 

Figure 1.5 shows that our model is quite accurate, with our predictions largely following the trend line. It can be seen that at higher sale prices our model does struggle, with the predictions looking much less accurate.

From our residual plots (1.6 and 1.7), we can see that our model struggles with predicting higher Sale Price houses especially at the $400,000 plus mark. This is likely due to there being very few houses at this price range, however it could also be explained by other factors. Higher end properties often possess unique characteristics that are difficult to capture with standard numerical variables, such as prestigous locations wthin neighbourhoods and premium finishes.The smaller sample size at this price point means our model has insufficient data to learn these complex patterns, resulting in larger prediction errors. Furthermore, the relationship between predictors and price may become non-linear at the extremes; for instance, an additional 500 square feet will likely add less value to an already large luxury home than it would to a mid-sized property. This suggests that future modeling efforts could benefit from either collecting more high-end property data or using separate models for different price segments of the housing market.



## Figure 1.5, 1.6 and 1.7 - Plots produced by our model:

```{r, echo=FALSE, message=FALSE, results='hide'}

library(dplyr)
library(ggplot2)

best_model_vars <- house_prices_filtered %>%
  select(GarageCars, BedroomAbvGr, TotalBath, BldgType, LotArea,
         Neighborhood, KitchenQual, TotalBsmtSF, OverallQual,
         ExterQual, YearBuilt, GrLivArea, SalePrice, Id)

set.seed(22)

n <- nrow(best_model_vars)
train_idx <- sample(seq_len(n), size = 0.8 * n)

train_df <- best_model_vars[train_idx, ]
test_df  <- best_model_vars[-train_idx, ]

predictors <- setdiff(names(best_model_vars), "SalePrice")

form <- as.formula(
  paste("SalePrice ~", paste(predictors, collapse = " + "))
)

model_80_20 <- lm(form, data = train_df)
test_predictions <- predict(model_80_20, newdata = test_df)

rmse <- sqrt(mean((test_df$SalePrice - test_predictions)^2))
mae  <- mean(abs(test_df$SalePrice - test_predictions))
r2_test <- 1 - sum((test_df$SalePrice - test_predictions)^2) /
               sum((test_df$SalePrice - mean(test_df$SalePrice))^2)

cat("Test RMSE:", round(rmse, 2), "\n")
cat("Test MAE:", round(mae, 2), "\n")
cat("Test R²:", round(r2_test, 4), "\n")

ggplot(
  data.frame(Actual = test_df$SalePrice, Predicted = test_predictions),
  aes(x = Actual, y = Predicted)
) +
  geom_point(alpha = 0.6, color = main_colour) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = accent_colour) +
  theme_house +
  labs(
    title = "Predicted vs Actual SalePrice (Test Set)",
    x = "Actual SalePrice",
    y = "Predicted SalePrice"
  )



residuals_df <- data.frame(
  Fitted = test_predictions,
  Residuals = test_df$SalePrice - test_predictions
)


ggplot(residuals_df, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.6, color = main_colour) +
  geom_hline(yintercept = 0, color = accent_colour, linetype = "dashed") +
  theme_house +
  labs(
    title = "Residual Plot (Test Set)",
    x = "Fitted Values (Predicted SalePrice)",
    y = "Residuals (Actual - Predicted)"
  )


ggplot(residuals_df, aes(x = Residuals)) +
  geom_histogram(binwidth = 5000, fill = main_colour, color = accent_colour, alpha = 0.7) +
  theme_house +
  labs(
    title = "Histogram of Residuals (Test Set)",
    x = "Residuals (Actual - Predicted)",
    y = "Count"
  )

```


# FINAL DISCUSSION

Through our findings, we are able to conclude that a small number of key housing attributes play a major role in shaping sale prices in Ames. Variables such as OverallQual, GrLivArea, GarageCars, and YearBuilt consistently showed the strongest relationships with SalePrice during our exploratory analysis. Using these predictors, we were able to fit a linear regression model that captured the main pricing trends reasonably well, achieving a moderate R² and producing predicted prices that aligned closely with the actual values for much of the dataset. However, the residual patterns indicated that a simple linear approach cannot fully represent the more complex, non-linear interactions present in the housing market. It is important to acknowledge the assumptions and limitations of our investigation. We treated missing values and categorical variables using simplified imputation and encoding approaches, and these choices may overlook subtle structure in the data. Our models also assume linearity and independence between predictors, which is unlikely to hold in real housing markets. While our model performed reliably on the selected predictors, more advanced techniques or additional feature engineering would likely produce stronger results. However, this model was made specifically for a very small part of the housing market, specific to Ames, Iowa. This means in the grand scheme of things this model may perform as well on a more generalised housing market in terms of a more wide scale range of places. Ethically, we recognise that our findings do not incorporate broader socio-economic or neighbourhood-level influences that affect real estate valuation. Oversimplifying model outputs or applying them outside the context of this dataset could lead to misleading conclusions, as housing markets are shaped by many external factors not represented here. 

## References

List any references here. You should, at a minimum, list your data source.


